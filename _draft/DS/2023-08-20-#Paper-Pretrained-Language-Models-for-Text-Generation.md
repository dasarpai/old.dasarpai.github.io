---
mathjax: true
id: 6088
title: Paper-Summary: A Survey Paper# Pretrained Language Models for Text Generation
date: 2023-08-18
permalink: '/dsblog/what-is-llm'
tags: [AI Paper, NLP, Pretrained Model]
categories:

header:
    teaser: /assets/images/dspost/dsp6088-Pretrained-Language-Models-for-Text-Generation.jpg
excerpt_separator: "<!--more-->"  
excerpt:  
layout: single  
author_profile: true  
toc: True  
toc_sticky: true
---

![Pretrained Language Models for Text Generation]( /assets/images/dspost/dsp6088-Pretrained-Language-Models-for-Text-Generation.jpg)

  
**Paper Name:- Pretrained Language Models for Text Generation: A Survey**  
Typer of Paper:- Survey Paper     
[Paper URL](https://arxiv.org/abs/2105.10311)    

# Paper Summary:- Pretrained Language Models for Text Generation

## Key Ideas from the Paper

- This paper discusses "major advances achieved in the topic of PLMs for text generation"
- This survey aims to provide "text generation researchers a synthesis" and pointer to related research.
- Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). 
- Neural generation model are deep learning models
- Pretrained language models (PLMs) are neural generation model

## Paper Outcome
- General task deﬁnition
- Describe the mainstream architectures of PLMs for text generation. 
- How to adapt existing PLMs to model different input data and satisfy special properties in the generated text. 
- Summarize several important ﬁne-tuning strategies for text generation. 

## Conclusion

Model Compression. Although PLMs with large-scale pa-
rameters have achieved success in text generation, these mod-
els are challenging to be deployed in resource constrained
environments. As a result, it is meaningful to study how
to achieve competitive performance with a small number of
parameters. Several methods have been proposed to com-
press PLMs, such as parameter sharing [Lan et al., 2020] and
knowledge distillation [Sanh et al., 2019], whereas most of
them focused on BERT-based models, and little attention has
been paid to compressing PLMs for text generation.

Fine-tuning Exploration: 
The direct intention of pretraining is to distill the linguistic knowledge learned in PLMs to downstream generation tasks. 
And, ﬁne-tuning is the predominant transfer method at present. There could be vari-
ous ways to transfer knowledge from PLMs to downstream
models. For example, Chen et al. [2020a] exploited knowl-
edge distillation by adopting BERT as teacher model and a
vanilla RNN generation model as student model. Through
this method, the linguistic knowledge of BERT can be dis-
tilled into the downstream model.

Language-agnostic PLMs: 
- PLMs for text generation are mainly based on English. These PLMs will encounter challenges when dealing with non-English generation tasks. 
- Language-agnostic PLMs are worthy to be investigated, which need to capture universal linguistic and semantic features across different lan-
guages. 
- An interesting direction is how to reuse existing English-based PLMs for text generation in non-English languages.

Ethical Concern: 
- Currently, PLMs are pretrained on largescale corpus crawled from the web without ﬁne-grained ﬁltering, potentially causing ethical issues such as generating private content about users. Therefore, researchers should try their best to **prevent misusing PLMs**. 
- Identifying threats and potential impacts and assessing likelihood. Ross [2012]
- The text generated by PLMs might be prejudiced, which is in line with the bias in training data along the dimensions of gender, race, and religion [Brown et al., 2020].
