Retrieval-Augmented Generation (RAG) combines 
- a retriever system, which fetches relevant document snippets from a large corpus, 
- and an LLM, which produces answers using the information from those snippets.

Objective: fine-tune an embedding model to improve performance of Retrieval Augmented Generation (RAG) systems over any unstructured text corpus (no labels required!).

RAG : Current State of Development

RAG is a popular paradigm for connecting Large Language Models (LLMs) with an external source of data that was not available during the training. It pairs a retrieval model over a knowledge bank with the LLM through its input prompt space. As of today typical RAG steps are as following:

- During Indexing: Prepare a corpus of unstructured text, parse/chunk it. Then embed each chunk and put in a vector database.
- During Query: Retrieve context from the vector db using top-k embedding similarity lookup, and stuff context into the LLM input space.

[LLaMaIndex Python Library](https://gpt-index.readthedocs.io/en/latest/getting_started/concepts.html) can be used to build LLM-powered application like chatbot, summarization, Q&A, FAQ etc. 

[Github: Fine-Tuning Embedding for RAG with Synthetic Data](https://github.com/run-llama/finetune-embedding#steps-for-running). It has code for
1. Generate a synthetic dataset. It is used for training and evaluation. generate_dataset.ipynb
2. Finetune a pretrained opensource embedding model. finetune.ipynb
3. Evaluate the finetuned model against e.g. the pretrained base embedding model and proprietary OpenAI embedding model.  evaluate.ipynb

As of today the biggest limitation of RAG is if the returned context is irrelevant to the query, then the capability of the LLM is irrelevant; the answer will always be bad.

Embeddings generated by pre-trained models may be close/far from each other based on the pre-training objective, but may not completely align with our own retrieval objective. For instance, if we building search over ML ArXiv papers, you may want the embeddings to align semantically with specific ML concepts (e.g. "Transformers", “LLMs”, “NLP”, "Encoder") and not filler words “This paper presents the...". The way to solve this problem is fine tune the embedding and not the model.

To finetune the embedding we need paired data as training examples. To avoid creating that manually, we can use LlamaIndex. These (question, chunk) pairs are then used as positive examples as training signals for the model (negative examples are randomly sampled across other chunks).