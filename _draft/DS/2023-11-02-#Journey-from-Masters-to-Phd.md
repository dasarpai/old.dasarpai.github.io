---
mathjax: true
id: 6098
title: My Journey from Master to PhD in Data Science and AI
date: 2023-09-03
permalink: /dsblog/journey-from-master-to-phd
tags: [Learning AI, Learning ML, Learning Process, PhD in AI, Question Answering with History Book]
categories:
header:
    teaser: /assets/images/dspost/dsp6098-Journey-from-MS-to-Phd.jpg
excerpt_separator: "<!--more-->"  
excerpt:  
layout: single  
author_profile: true  
toc: True  
toc_sticky: true
---

![My Journey from Master to PhD in Data Science and AI](/assets/images/dspost/dsp6098-Journey-from-MS-to-Phd.jpg)

# My Journey from Master to PhD in Data Science and AI

I have been in software development between 1993 to 2009. Some of these years were in senior leadership roles in delivery management, project management, CMMI, ISO, ISMS, PMO etc. In 2010 I moved into project management training and consulting. In 2018, I was considering to go back to technology but this time I wanted to pickup completely new stack of technology. I decided, I will move into Data Science and AI. I knew AI as much as any typical software development person in senior management know about this. It means, I was highly confident that I can pick this up quickly. On top of that lots of content is available on internet, many YouTube channels, and many courses were available. I thought it should be cake walk for me. I started learning this technology in my own way in the committed free time. Within 5-6 months after lots of study I started I realizing this the way to get the knowledge but it cannot help me getting confident to solve the problem. In fact more I am learning, more I am feeling I do not know how much I need to learn or how far I need to go.

That time after a lots of contemplation I decided I must make long term, time and money commitment for learning new technology. Otherwise I will know many jargons of this but never will get confidence to solve the problem using this, and I will not be able to appreciate the future scope of AI technology and avenues available. I took admission into M.S. in Data Science program, this was a joint program offered by IIIT Bangalore and LJMU (Liverpool John Moore University). Including our pre-course preparation + course + post course evaluation, it took around 2 years. I learned so many things that it changed my perspective about AI, Machine Learning, Deep Learning, Robotics, IoT, BigData and impact on the business.

I am extremely enthusiastic about utilizing AI technology to address issues related to Indian languages. This is precisely why I chose to base my MS thesis on the topic of "Sarcasm Detection in Hinglish Language (SDHL)." However, during this period, the NLP landscape experienced rapid changes due to the emergence of Transformer technologies and Large Language Models (LLM). Despite having numerous intriguing projects in mind that revolved around Indian culture and languages, I faced challenges such as time constraints, funding requirements, and the maturity of the technology. Consequently, I became fascinated by the Question-Answer task in NLP and decided to focus on applying it to Indian historical books. Thus, I embarked on my journey by selecting "The Mahabharat" as the book of interest.
 
I confess following on this journey. And suggest anyone who want to embark on this journey be ready for all this at least if not more. If you feel better than what I am writing below then you can consider yourself lucky enough.

- It was completely lonely journey, nobody will be with you in this journey. 
- Need to learn and master a programming language to code for data scrapping, data cleaning, model training, model evaluation, data analysis. I remain focused on Python.
- Learn NLP technology from scratch. From basic nuance of any language like part of speech, lemmatization, grammar to text embedding, neural network, and mathematics behind that.
- Transformer technologies
- Large Language Models
- Usage of different databases SQL Database, Non-SQL database, Vector Database and Graph Database.
- How to select relevant research papers, how to read technical papers, making notes, citing them properly
- Finding gaps in papers, finding improvement opportunity in the paper.
- Finding what trick, technique from a paper can be used or refined for my work.
- Learning LaTex
- Learning Research Management 
- Learning NLP Model building and finetuning. 
- Learning NLP Model Evaluation, especially for NLP and Question Answering Work.
- Cloud Platforms available from Google (VertexAI), Amazon (AWS Sagemaker) and Microsoft (Azure AI Machine Learning)

**In this journey I learned so many things which is not possible to sum up or enumerate in one blog. During this journey I wrote dozens of articles and, in the interest of brevity, in this article linking to those different articles.**

## DBA vs PhD Difference and Similarity
Learning what can be called research and how to conduct serious research is important of any career. Initial period of our career  we do whatever we learned from college, institutions, colleagues, organization training. If you learning seriously then a time come when want solve the problems with new methods. Those new methods may be efficient, produce better quality, more economic, more safe and secure, environment friendly, etc. But, when we have are allowed to lead those initiatives, many times we feel stuck in the well. It is easy to start some idea, to take it to conclusion demands a different rigor.

This rigor come to you from academic rigor. With my experience in software development, process improvement, project management training and consulting I am saying this seriously that this rigour doesn't come without go through some serious project. Doing some research project takes different kind of mindset. In research project, sponsor spends money to learn the ways how problem can be solved and at the same time it is equally important how the problem cannot be solved. We need to demonstrate this emperical data.

If you serious to learn about mindset then PhD is way, it gives you that rigor. Ph.D is wonderful and rigrous but the issue is after one or two decade in the industry, when you get maturity and you can solve the problem then you don't want work university professor to learn this methodology that too at the cost of loss of salary and rejections from university professors. Most of the time it becomes ego fight between established unisersity professor and professionals. Ph.D make more sense when you are continuing your master or within first 5 years of your career. After that there are lessor gain and more loss and that is the reason extremely low number of people from corporate choose this path.

There is an alternative path for the experienced working professionals. That path is DBA, Doctrate in Business Administration. Do not confuse this with typical admission administration degree. Either you have domain extertise or technology expertise, if you want to take those to next level for innovation and research then DBA makes completely sense. Good things is here your guides are from the industry and not from university. So, they understand you better than academic university professors. On top of that, if you are lucky enough you may find a guide who is more experience in that domain, in which you want to pursue your DBA. If you are more lucky then you get a research partner who is working on the same topic and this person may be your research guide or other fellow student.


# Learning 
I would like to summarise my learning in this journey in three parts
1. Learning Related Conducting Research in AI/NLP Domain
2. Learning Related to AI/NLP Technologies
3. 

## Research Related Learnings

### How to Conduct AI Research?
Depending upon your domain, technology type in which you are doing research, there can be different ways of conducting research. Some popular research types are 
- Basic Research (Pure Research)
- Applied Research (Most of NLP related researchers falls under this group). It provide solutions and is often conducted with a clear end goal in mind, such as developing new technologies or improving existing processes.
- Quantitative Research 
- Qualitative Research 
- Descriptive Research: Without giving any conclusion, just document the observations of different experiments.
- Causal Research 
- Longitudinal Research: Longitudinal research involves collecting data from the same subjects over an extended period.
- Action Research 
- Case Study Research 

The main flow of conducting research is as follows:
- Define problems clearly: Don't pick things just for degree. Pickup some topic which you care for, for which data is available or you can get, for which you can put extra efforts, go extra mile. Otherwise in the long journey you interest will come down and like many other students you will drop the idea.
- Ask yourself what do you know about the existing state problem and existing solutions?
- Define, what contribution your work will do? You don't solve that problem then what will happen? If answers are not giving kick to you then don't go ahead, simply drop the idea.
- Articulate your research questions, which you must answer by the end of research.
- Articulate your hypothesis, assumptions, limitations clearly.
- [Conduct literature survey](/dsblog/How-To-Do-Litrature-Review). I have written a separate article on this. 
- In self funded data science research, avoid the temptations of using huge data or using LLM. You may not get the resources or you may endup paying a lot of money from your own pocket in this journey.
- Visulise the raw data, clean the data and visualize. It will help you understand whether data is good enough for training purpose.
- Master a programming language, if you donot know then you have learn, otherwise it will cause you lot of pain during experimentation. No matter how many coding tools, coding assistance, great IDE, libraries are available there is not replacement of your own ability to write and debug.
- Document every setup and outcome of every setup. If you don't like some of the results then don't throw them away. Document everything thing. Remember, research is not only about what works, it is also about what doesn't work.
- Analyze your research data and ask yourself whether your research questions can be answered? What happend to your null hypothesis?
- Write your conclusions, contributions made, possible future improvement, something which you couldn't do and why?

### Research Project Management
I am coming from project management consulting background so I understand that the any project is about change management, a project is an unique endeavor or produce unique output. These outputs can be product, services of results. The success of any project depends upon understanding enterprise environmental factors, organization process assets, project lifecycle, project resources and ten aspects of any project namely: scope, schedule, cost, quality, project resources, communication, risk, procurement and integration. So, research project is not any different from any other normal project in these sense. Because they also have resource, Gantt chart, quality, scope, requirements, risk, and other important things to take care.

But we need to understand research project are unique in the sense of trying to understand business problem in detail or trying to understand possible solution around that. During research project we don't solve the problem fully, but layout a clearly defined path following which problem can be solved. So the output of research project is prototype, model, high-level or detail approach, methods to solve the problem, a reports which can explain the problem in a better way. In research project you are focused on creating solution but more than that you interested in knowing all the possible ways to solve the problem, what are pros and cons of each methods. These pros and cons can come in the form of certain metrics which influences, cost, time, quality, efficiency etc.

It is possible that in the research project you need to establish new metrics to evaluate performance of the final research outcome. In my case, I was working on descriptive question-answering work. How to evaluate questions and answers generated by the system are correct? what is the meaning of correct? How much correct they are? If your problem is unique you may deviate from standard metrics.

Research is creative work, tracking the research work and knowing how much work has been done is not that straight forwards as in typical construction projects or maintenance project or software development. Every project type has certain degree of creativity involved in their more the creativity more uncertainty around marking a task is complete or not. One day you can mark a task is complete and after one week you get some new idea to solve that problem and you start working on the same task, which was marked complete. Sometime you realize dataset was incorrect or we have different or better dataset and you use that dataset on the solution completely last week. And now it is failing, you enter into another loop.

Sometimes months will go in reading, thinking and no break through and one day all of sudden you realize solution is before you eyes. Due to these reasons, it is extremely important you document your project scope, research questions, assumptions, limitations, risks, related work after thorough thinking.

Finally, depending upon organization size, priorities, strategies, market position, industry research budget may vary. Either individual or organization generally they don't keep enough budget for research. Due to this reason in research project we have scarcity of resources, you may be along or hardly one or two people with you. On top of this cycle difference between research and productization of research may be very small. This can lead to lots of pressure on research team to improvise the product. Every organization has different ways of handling this challenge in different way.


## NLP Related Learnings

## NLP Terminologies 
I created this resource and keep updating it. [Comprehensive Glossary of LLM, Deep Learning, NLP, and CV Terminology](/dsblog/comprehensive-glossary-of-llm) is my understanding around some important terms.

# NLP Tasks
There are hundreds of tasks in NLP which can be done using various NLP technologies. I wrote an article, grouping and listing these task.
[NLP Tasks](/dsblog/nlp-tasks)

# NLP Evaluation 
I was struggling to evalute the performance of different sub-systems of my models. My work was purely on NLP but I came accross many metrics which were worth documenting at one place. I wrote [this article](/dsblog/machine-learning-metrics) on ML learning model evaluation. Many of these metrics which are mentioned I used in my research project. I used BLEU Score, ROUGE Score, Cosine, NLP Precision, NLP Recall, NLP F1_Score, R@n, P@n, MRR and MAP metrics for evaluating different subsytems in my project.

Another related article is [Distances in Machine Learning](/dsblog/Distances-in-Machine-Learning)

## Transformers Models for Question-Answering
- Models for Extractive Question Answering Task: BERT, DistilBERT, ALBERT, RoBERTa by Meta, XLNET, GPT3, 
- Models for Generative Question Answer Task: T5 by Google, GPT3, DialoGPT, CTRL, Encoder-Decoder Models.
- LLM for Question Answering Task: GPT3, GPT-Neo, ChatGPT and GPT4 by OpenAI, BLOOM by Huggingface, LLaMa and Jurassic-1  by Meta, Chinchilla, LaMDA & PaLM by Google, XLNet by CMU and Google, Megatron-Turing NLG


## Large Language Models 
A large language model (LLM) is a type of artificial intelligence (AI) model. These are designed to understand and generate human language, they also have other capabilities like image, video, sound, voice unserstanding and generation. They are trained on vast amounts of text data to learn the structure, comprehend differnt parts of the input, grammar, and semantics of language. Large language models are typically based on deep learning architectures, such as recurrent neural networks (RNNs) or transformer architectures like encoder, decoder, encoder-decoder. They are called large because they are trained on massive datasets containing text from the internet. This extensive training data helps the model capture a wide range of linguistic patterns and knowledge. In this training process the model learns coeffients of billons of parameters.

Large language models are typically pre-trained on a diverse corpus of text. During pre-training, the model learns to predict the next word in a sentence, which helps it develop a strong understanding of language. After pre-training they are finetuned for many other specific tasks and in this process we create many experts. ChatGPT syte

Fine-tuning: After pre-training, the model can be fine-tuned for specific tasks, such as text generation, question-answering, or translation. Fine-tuning tailors the model's capabilities to the desired application.

Versatility: Large language models are versatile and can be used for various natural language processing (NLP) tasks, including text generation, language translation, sentiment analysis, chatbots, and more.

Human-like Text Generation: These models are known for their ability to generate coherent, contextually relevant text that can be difficult to distinguish from human-written content.

Semantic Understanding: Large language models can understand context, answer questions, and complete text based on the input they receive. They can also perform tasks that require reasoning and knowledge retrieval.

Examples of well-known large language models include GPT-3, developed by OpenAI, and BERT (Bidirectional Encoder Representations from Transformers), developed by Google. 


I wrote a separate detailed article on [Large Language Models (LLM)](/dsblog/what-is-llm)


## Text Embedding Technologies
- Word Embedding: GloVe, Word2Vec, TFIDF
- Sentence Embedding: Doc2Vec, SentenceBERT, InferSent, USE (Universal Sentence Encoder), SentenceTransformer 
- [Embedding with FastText](/dsblog/embedding-with-fasttext)

## Vector and Graph Databases 
Vector databases are specialized storage systems developed for efficient management of dense vectors. They differ from standard relational databases, such as PostgreSQL, which were built to store tabular data in rows and columns. Theyâ€™re also distinct from newer NoSQL databases like MongoDB that store data as JSON. Vector databases are designed to store and retrieve just one type of data: vector embeddings. Vector embeddings are the distilled representations of the training data produced as an output from the training stage of the machine learning process. They serve as the filter through which fresh data is processed during inference.

- Pinecone: A managed, cloud-native vector database with a straightforward API and no infrastructure requirements. I wrote this article on [Pinecone](/dsblog/What-is-Pinecone)
- Milvus: An open-source vector database designed to facilitate embedding similarity search and AI applications.
- Chroma: An open-source embedding database that excels at building large language model applications and audio-based use cases 
- Weaviate: An open-source knowledge graph that allows users to store and search for data objects based on their semantic meaning.
- FAISS: A high-performance vector database capable of supporting real-time applications, session management, and high-traffic websites

## Complexity Around Question-Answering Generation Task
- I wrote a separate artile on [Types of Questions.](/dsblog/types-of-questions). It discusses different types of question, why People ask questions?
- Challenging is solving question answering task using AI technology.
- Do we want to generate questions only or we want to genearte question and corresponding answers or we want to generate question along with corresponding answer and reference test?
- What kind of question we want to create, descriptive, boolean, multi choice (MCQ)?

## Complexity Around Domain Like History 
- Grammer of the age, for example, written english of 18th century has follows different grammer than today's english.
- Spelling of words (name of people, name of places, name of festivals, name of plants) from the original old text. For example Mahabharat translation from Sanskrit to Mahabharat will have inconsitent spelling for words. This is transcription issue.
- Context of era is important, for example if you mix Ramcharit manas text with preindendence history text then many things looks out of context and combining them can create problem
- The old text may be biases towards some gender, caste, religion, geography, profession etc.
- Format of the old book, especially old Sanskrit work is in the form of Sutra. You need to careful whether you are using translation of sutra or translating commentary on sutra. 
- If you are using tranlated history work then you need to careful about the acceptablity of the tranlation by wider readers
- If you are using translated history work then you also need to be careful the translation is created from original text or from translated work. For example, we pickup English translated work of Mahabharat, now if we want to pick Hindi translation then this was created from English work or from original Sanskrit work.
- Do we have word embedding and sentence embedding around the text in hand?


## Literature Survey
- My experience in Litrature Survey & Reivew. [How to conduct Literature Survey](/dsblog/How-To-Do-Litrature-Review)
- Window shopping of the work done by other researchers in your area of interest.
- How to identify important work?
- What to read in identified work?
- Selecting work for second reading. What to read in second reading?
- Why to use other's work?
- How to use other's work? Different style of quoting the work.
- How to avoid citation errors?
- Different kind of papers.
	- Conference Paper 
	- ArXiv Paper 
	- Book 
	- Thesis 
	- Journal Article
	- Magazine Article 
	- Report 
	- Bill/Act
	- Patent
	- Working Paper
	- Encyclopedia Article
	- Paper Published in Journals
- Relationship between Publishers, Journals, Domain, Volumes, Articles
- List of popular AI/NLP publishers and journals
	
## Publishing (Paper/Book/Article)
- Publication via
	- Thesis (Phd/Longer) and Desertation (MS/Shorter) 
	- Magazine Article 
	- Journal Article
	- Conference journal
	- Newletter
	- Online Journals
	- Book
- Publishers
	- [Association for Computational Linguistics (ACL)](https://www.aclweb.org/)
	- IEEE (Institute of Electrical and Electronics Engineers)
	- [Elsevier](https://www.journals.elsevier.com/)
	- [Springer](https://www.springer.com/ai)
	- ACM (Association for Computing Machinery
	- MIT Press
	- Cambridge University Press
	- O'Reilly Media
	- [Springer Nature](https://www.springernature.com/gp)
- Popular journals to publish AI/NLP work. 
	- Journal of Artificial Intelligence Research (JAIR)
	- Artificial Intelligence (AI) Journal
	- Computational Linguistics (CL) Journal
	- Natural Language Engineering Journal 
	- ACM Transactions on Speech and Language Processing (TSLP)
	- IEEE Transactions on Neural Networks and Learning Systems:
	- Journal of Machine Learning Research (JMLR)
	- Journal of Artificial Intelligence and Research in NLP (AIRNLP)
	- NeurIPS Proceedings (Conference on Neural Information Processing Systems)
	- EMNLP (Empirical Methods in Natural Language Processing)
- Popularity and impact factor (IF) of journals are important parameter to consider before publishing. But they keep changing over time.
- CRediT for author contribution role. Credit Texonomy. It helps in funding compliance. Main roles are - Conceptualization, Data Curation, Formal Analysis, Funding Acquisition, Investigation, Methodology, Project Admin.
- Scopus is an abstract and citation database that provides access to a vast collection of academic and scientific research literature. It is one of the most comprehensive and widely used bibliographic databases in the world. Scopus is particularly popular in the academic and research communities and is frequently used for literature review, citation analysis, and research evaluation.

## Thesis Documentation
- Different components of thesis and their order.
	- Title 
	- Dedication
	- Acknowledgement 
	- Abstract 
	- Abbreviations 
	- Table of Contents
	- List of Tables 
	- List of Figures 
	- Thesis Chapters 
	- Appendix 
	- Bibliography 
	- Index 
- Page numbering format for main content and non-main content 
- Thesis report formats
- Different bibliography formats like Harvard, APA, IEEE AMA, APSA, ACS etc.
	- How the items will be listed in bibliography (sorting order, author first name, middle name, last name, year of publication, article name, order of in the text string)
	- How the items will appear within the text (subscript, number, first author last name)
	- How the items in bibliography appears (in the order they appear in the text or firtst author last name or something else)
	- Use of [] or () around name, number of the corresponding bibilography information, when it appears in the main text.

## LaTex Learnings
- There are many LaTex compiler, many Front End and online software.
- Popular Online: Overleaf, Popular desktop: TexStudio.
- Different LaTex compilers, LaTeX PDFLaTeX, LuaLaTeX, XeLaTeX
	- PDFLaTeX: Well-supported and compatible with a wide range of LaTeX packages and styles. Good, If you don't need advanced font features or complex scripting.
	- LuaLaTeX: Suitable for documents that require advanced font features or have specific Lua scripting needs. It can also execute Python code within the script.
	- XeLaTeX is designed to support Unicode and OpenType fonts, making it an excellent choice for multilingual documents with complex font requirements. Built-in support for languages and scripts that use non-Latin characters
- LaTex packages for using macros
- CTAN stands for the "Comprehensive TeX Archive Network." It is a central repository of free and open-source materials related to the TeX typesetting system and its extensions, including LaTeX. CTAN provides a comprehensive collection of TeX-related software, packages, fonts, documentation, and other resources.
- TeX Live and MiKTeX provide a user-friendly way to manage LaTeX packages and keep your LaTeX distribution up to date.
- Different LaTex document type, i.e. Article, Report, Thesis, Book
- Different page orientation in LaTex and changing oridentation between different sections.
- Multi column layout
- Document Structure: Chapter, Section, Subsection, Subsubsection
- Organizing dozens of images, tables, appendices and main chapters of multichapter book.
- Drawing images in LaTex
- Hyperlinking, internal and external links
- Enumeration and Itemization with different format 
- Writing math, calculus, statistics and chemistry formula in latex
- Table formatting, column merging, longtables, overflowing tables 
- Image formatting, image size, image location, image format, image dimension, image resolution 
- Using multiple language script like English, Devanagari, Kannada, Tamil in LaTex document 
- Footnote 
- Creating and using Glossary 
- Creating and using Index 
- Creating Table of Content
- Creating List of Figures 
- Creating List of Tables
- Using Citation and Citation Styles
- Creating Bibliography and citation 
- Making sure appropriate page margins and table, figure appearing at place and in order, it is intended. 

## Working with Limited Resources 
Generally in research projects we are constraint by budget and it is more true in academic research like DBA and Ph.D. I was working on AI-NLP project and wanted to explorer capabilities of transformers and LLM. Therefore there are two kinds of resources needed. One, API services from ChatGPT and ChatPDF and other companies. Second, GPU machine, which can be used for model training and inferencing purpose. Both of these are expensive resources and if you are not careful in resource planning or exploring free resources or running experiments with leisurely you may burn up thousands of dollars. What specific API, with how much data, what is the output, how much GPU we need, what are options available in the market and at what price. We need to explore all these options carefully so that you can complete the work without spending any money (with free resources) or least money. Unfortunately getting the answer of these questions is not that easy. The the main reason for that initially, question are not clearly framed and secondly internet is full of different kind of options you need to pay attention and choose carefully.


## Questions Answering System for Big System 
When ChatGPT is available then why need to spend time and money in creating question answering system? ChatGPT can create question from the text which available in the public domain. Similarly ChatGPT can answer those question for which text is available in the public domain. But, even in the public domain, can it answer questions from old Sanskrit or Tamil text? For that we need to understand what corpus is used to create ChatGPT. ChatGPT, being a propriety commercial tool they have not published anything valuable about this model. What text they have used? How many paramteters model? How many hardware of what kind has been used for training etc. this information is not available. So we don't know what answer can be given by ChatGPT, what question can be generated by it. Secondly if some data is your private data, like health, income, email, WhatsApp msg, corporate data, government data etc., this data is not available in the public domain. If we want to create a QA system around this then we need to go away from ChatGPT and we need our own systems.

What technology can be used, in terms of hardware, software, network, security for model training and inferencing, that depends upon volume of data and kind of data. Data may any kind like tabular data like csv files of excel sheet, RDBMS data, image data, health image (x-ray, MRI etc), text data (book, article, news, email etc), video (gif, short clip, live stream, movies, surveillance recording), audio (conversation, music, singing, speech, etc). In my case I was dealing with text data. English translation of Mahabharat book.

We need to understand how to perform question answering work when you have thousands of book, emails, corporate circulars, procedure documents, and these documents have information in different formats.

If we explore LLM like LLaMa, BLOOM, PaLM, GPT3 etc. then how to finetune these models with limited resources? I explored PEFT and LoRA with LLaMa and BLOOM. We also need to keep in mind model training became easy it does not mean inferencing can be done on low grade hardware.


## Application and Scope of Question Answering System (QAS)

Wisdom come when we contemplate, process data, ask questions. Deeper the quality of question, more challenging and unique those questions are, more unshakeable wisdom you will have. We need to understand after reading a book asking question is one thing, without reading the question what questions we can ask? After question is answer how do you know the question is answered correctly or not? This is completely different challenge and it more challenging for computer and much more challenges for computer when the answer is generated from the different part of the text and extract from one or two parts of the text. Whatever is the domain humans and questions so QA is ubiquitous domain agnostic language task. QAS can take many format, for example chatbot, FAQ, interview, searching, exam evaluation, interrogation, exploration etc. With a few example we will try to understand applications of QAS.

- When, at the end of university of school want to evaluate learner.
- When, doctor what to examine the patient.
- When, auditor want to know the answer of a question.
- When, journalist want to ask some question about politician.
- When, sales person want to know why the sales less in last quarter.
- When, HR want to know whether a policy is violated.
- when, CEO, want to understand, what factor is contributing how much in the cost of a product.
- when, listener want to validate his/her understanding about a text.


## Learning Python
- Decorator function
- Building Utility Libraries 
- List comprehension with Lambda function.
- Using GPU with pytorch and tensorflow.
- Google colab forms 
- Using streamlit library for frontend 
- NLP Library: SpaCy, NLTK, Gensim

## Web Scrapping 
- While exploring different web scrapping techniques and libraries I wrote this article [Python API for Data Collection](dsblog/python-apis-for-data)
- Apart from this I web scrapped 2100 Sections of Mahabharat Books from 2100 urls for my DBA.
- During my MS program I used several other web scrapping tools to scrap data from twitter and other social media accounts.

## Configuration Management & Tools in Data Science/NLP Research Projects 
- Report Writing: LuaLaTeX, Overleaf,
- Coding: Python/ Jupyter Notebook
- Code Version: Github 
- Workflow Design, Presentation: Google Docs
- Notes during Exploration: Notepad++
- Citation Management: Mendeley
- Document Storage: Google Drive
- Research Exploration: ChatGPT, Google Scholar, ResearchGate




## Prompt Engineering
I wrote [this](/dsblog/introduction-to-prompt-engineering) detailed article about Prompt Engineering (PE). What is PE and what are the possibilities. I used ChatGPT for prompt engineering whenever I needed a partner and guide to ask questions/doubt/clarification about following.

- Asking python programming questions
- Debugging python code
- Asking research guidance 
- Asking for options and evaluation of options
- Asking for summarization 


## Model Finetuning & Training
- [Model Turning with VertexAI](/dsblog/Model-Tuning-with-VertexAI)

## Model Deployment 
- [Introduction to AI Model DeployementPermalink](/dsblog/introduction-to-ml-model-deployement)

# Model Repositories
- [AI Model Zoo](https://dasarpai.com/dsblog/introduction-to-ml-model-deployement#ai-model-zoos)
- [ML Respository from Pinto](/dsblog/ML-Model-Repository-from-Pinto0309)
- [Model Garden of VertexAI](/dsblog/Model-Garden-of-VertexAI)

## Other Learnings
- Langchain, Chain of Thought, Tree of Thought
- Data Scrapping from Website
- Ethics Related Issue in AI Products 

## Technologies 
- NLP Model Finetuning
- LLM Model Finetuning 

# Some useful resources

- [12 Vector Databases For 2023: A Review](https://lakefs.io/blog/12-vector-databases-2023/)
