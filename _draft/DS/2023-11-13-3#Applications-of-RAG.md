---
mathjax: true
id: 6104
title: Applications of RAG
date: 2023-11-11
permalink: /dsblog/application-of-rag
tags: [NLP, NLU, LLM]
categories:
header:
    teaser: /assets/images/dspost/dsp6104-Applications-of-RAG.jpg
excerpt_separator: "<!--more-->"  
excerpt:  
layout: single  
author_profile: true  
toc: True  
toc_sticky: true
---

![Applications of RAG](/assets/images/dspost/dsp6104-Applications-of-RAG.jpg)

# Applications of RAG

n e-commerce company wants to empower their product-return handlers to have interactive "Chats" with policy documents to expedite refund decisions. In order to harness the power of longer contexts, they'd have to train (not merely fine-tune) models on their exclusive data. This could be very costly and time-consuming affair, even with just 2k context window.

enerating embedding representations is the GPT-J-6B model.

we actually don't even necessarily need a large model to create an embedding for your query and documents.



