Nowadays with a flod of AI models in the market there is abundance of AI models in the market. Some of these are proprietry and some are opensource. For the properietry models like Gemini, Claude Sonnet, ChatGPT you need to go to their provider's website use the model and pay them for the usage. If you are app developer then you can use their API integrate them in your application and pay for that the usage. Now, because you have AI product and you have some customer of your product therefore based on the usage your customer pay to you and you pay to model provider. Having a business model when you are earning from your customer and paying to AI Model API provider posses one kind of challenge when the capabilities of these model is continuously changing and new players are emgering every week if not every day. But if you are AI researcher who is helping your client in selecting these models so that they can use these either as API or frontend that possess different kind of challenges. Do you want to subscribe all AI providers and pay to all to evalute their product? Some rich may choose that path. And this looks inevitable in the case of propritry models.

But, what you open source models. These model can run on my local infrastructure. I may use these models for research purpose, benchmarking purpose or creating a service within one department of my organization or may for the entire organization. To use these models you need hardware, but AI hardware is very expensive some people are ready to do that investment but they need to determine what is the limit? How big infrastructure you need? Either for research purpose or for production purpose when market is extremely volatile because many reasons like competition, government regulations, taxation, safety concerns, geo-politics, melting stockmarkets etc what investment you should make for AI infrastructure is extremely difficult to determine.

As an indendent AI researchers who helps my customers in using right model for a given task, I feel we need to take mix approach. You can do mid level investment in a GPU machine like Geforce 4070, 4090, the best 5090. Each of these have their own capability and corresponding cost. Depening upon when you are reading this arcile price and relevence of these machine may change.

With any of these machines and OLLAMA app you experiement with many of the opensourced model. If needed you can try quantaised models as well. Now, if those experiments cannot run on your local machine then go for cloud. But keep in mind, that cloud providers should be aggregator of AI models, it means without reinvesting you should be able to switch between different models. And all this should happen smoothly, without much change in your existing code. Now, because you are using AI cloud for experimentation you need to keep in mind that it should allow to get dataset from anyother cloud or load your datasets. The infrastrcuture should support model finetuning and obserbablity. For obsevablity I may use their service or other free service provider.

