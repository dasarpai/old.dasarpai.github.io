---
mathjax: true
id: 6116
mathjax: true
id: 6133
mathjax: true
id: 6133
mathjax: true
id: 6133
mathjax: true
id: 6133
mathjax: true
id: 6133
mathjax: true
id: 6133
mathjax: true
id: 6133
mathjax: true
id: 6133
title: 6104#Applications of RAG
date: 2023-11-11
permalink: /dsblog/6104#Applications-of-RAG
tags: [NLP, NLU, LLM]
categories:
header:
    teaser: /assets/images/dspost/dsp6133-6104#Applications-of-RAG.jpg
excerpt_separator: "<!--more-->"  
excerpt:  
layout: single  
author_profile: true  
toc: True  
toc_sticky: true
---

![6104#Applications-of-RAG](/assets/images/dspost/dsp6133-6104#Applications-of-RAG.jpg)

# 6104#Applications of RAG


# 6104#Applications of RAG


# 6104#Applications of RAG


# 6104#Applications of RAG


#6104#Applications of RAG


# Applications of RAG

n e-commerce company wants to empower their product-return handlers to have interactive "Chats" with policy documents to expedite refund decisions. In order to harness the power of longer contexts, they'd have to train (not merely fine-tune) models on their exclusive data. This could be very costly and time-consuming affair, even with just 2k context window.

enerating embedding representations is the GPT-J-6B model.

we actually don't even necessarily need a large model to create an embedding for your query and documents.



