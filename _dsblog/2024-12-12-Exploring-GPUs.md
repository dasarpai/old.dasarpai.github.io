---
mathjax: true
id: 6188
title: "Exploring Graphics Processing Units (GPUs)"
date: 2024-12-12
permalink: /dsblog/Exploring-GPUs
tags:
  - AI
  - ML
  - GPUs
  - NVIDIA
  - GPU Architecture
categories:
  - dsblog
header:
  teaser: /assets/images/dspost/dsp6188-Exploring-GPUs.jpg
excerpt_separator: "<!--more-->"
author: Hari Thapliyaal
layout: dspost-layout
excerpt: "Exploring the journey, capabilities, architecture and components of Graphics Processing Units (GPUs)"
author_profile: true
share: true
toc: false
toc_sticky: true
toc_levels: 3
comments: true
keywords: ["gpu architecture for artificial intelligence", "nvidia gpu for machine learning", "gpu deep learning applications", "gpu in ai research", "gpu for computer vision", "gpu for natural language processing", "gpu for robotics"]
---

![Exploring Graphics Processing Units (GPUs)](/assets/images/dspost/dsp6188-Exploring-GPUs.jpg)

# Exploring Graphics Processing Units (GPUs)
 
## **Overall Computational Power of GPUs**  
- âš¡ **Incredible Calculation Speed:** Modern GPUs can perform tens of trillions of calculations per second (e.g., 36 trillion for Cyberpunk 2077).  
- ğŸŒ **Human Comparison:** Achieving this manually would require the equivalent of over 4,400 Earths full of people, each doing one calculation every second.
 
## **GPU vs. CPU**  
- ğŸš¢ **Cargo Ship vs. Airplane Analogy:** GPUs are like cargo ships (massive capacity, slower), and CPUs are like jets (fast, versatile, fewer tasks at once).  
- âš–ï¸ **Different Strengths:** CPUs handle operating systems, flexible tasks, and fewer but more complex instructions. GPUs excel at huge amounts of simple, repetitive calculations.  
- ğŸ”€ **Parallel vs. General Purpose:** GPUs are less flexible but highly parallel, CPUs are more general-purpose and can run a wide variety of programs and instructions.
 
## **GPU Architecture & Components (GA102 Example)**  
- ğŸ’½ **Central GPU Die (GA102):** A large chip with 28.3 billion transistors organized into Graphics Processing Clusters (GPCs), Streaming Multiprocessors (SMs), and cores.  
- ğŸ—ï¸ **Hierarchical Structure:** GA102 has 7 GPCs â†’ 12 SMs per GPC â†’ 4 Warps per SM â†’ 32 CUDA Per Wrap and 4 Tensor Per Warmp and 1 Ray Tracing Per GPC.  
- ğŸ”¢ **Types of Cores:**  
  - âš™ï¸ CUDA Cores: Handle basic arithmetic (addition, multiplication) most commonly used in gaming.  
  - ğŸ§© Tensor Cores: Perform massive matrix calculations for AI and neural networks.  
  - ğŸ’ Ray Tracing Cores: Specialized for lighting and reflection calculations in real-time graphics.
 
## **Manufacturing & Binning**  
- ğŸ”§ **Shared Chip Design:** Different GPU models (e.g., 3080, 3090, 3090 Ti) share the same GA102 design.  
- ğŸ•³ï¸ **Defects & Binning:** Manufacturing imperfections result in some cores being disabled. This leads to different â€œtiersâ€ of the same GPU architecture.
 
## **CUDA Core Internals**  
- â• **Simple Calculator Design:** Each CUDA core is basically a tiny calculator that does fused multiply-add (FMA) and a few other operations.  
- ğŸ’» **Common Operations:** Primarily handles 32-bit floating-point and integer arithmetic. More complex math (division, trignometry) is done by fewer, special function units.
 
## **Memory Systems: GDDR6X & GDDR7**  
- ğŸ’¾ **Graphics Memory:** GDDR6X chips (by Micron) feed terabytes of data per second into the GPUâ€™s thousands of cores.  
- ğŸš€ **High Bandwidth:** GPU memory operates at huge bandwidths (over 1 terabyte/s) compared to typical CPU memory (~64 GB/s).  
- ğŸ”¢ **Beyond Binary:** GDDR6X and GDDR7 use multiple voltage levels (PAM-4 and PAM-3) to encode more data per signal, increasing transfer rates.  
- ğŸ—ï¸ **Future Memory Tech:** Micron also develops HBM (High Bandwidth Memory) for AI accelerators, stacking memory chips in 3D, greatly boosting capacity and speed while reducing power.
 
## **Parallel Computing Concepts (SIMD & SIMT)**  
- â™»ï¸ **Embarrassingly Parallel:** Tasks like graphics rendering, Bitcoin mining, or AI training are easily split into millions of independent calculations.  
- ğŸ“œ **Single Instruction Multiple Data (SIMD):** Apply the same instruction to many data points at onceâ€”perfect for transforming millions of vertices in a 3D scene.  
- ğŸ”“ **From SIMD to SIMT:** Newer GPUs use Single Instruction Multiple Threads (SIMT), allowing threads to progress independently and handle complex branching more efficiently.
 
## **Thread & Warp Organization**  
- ğŸ“¦ **Thread Hierarchy:** Threads â†’ Warps (groups of 32 threads) â†’ Thread Blocks â†’ Grids.  
- ğŸ›ï¸ **Gigathread Engine:** Manages the allocation of thread blocks to streaming multiprocessors, optimizing parallel processing.
 
## **Practical Applications**  
- ğŸ® **Video Games:** GPUs transform coordinates, apply textures, shading, and handle complex rendering pipelines. Millions of identical operations on different vertices and pixels are done in parallel.  
- â‚¿ **Bitcoin Mining:** GPUs can run the SHA-256 hashing algorithm in parallel many millions of times per second. Though now replaced by ASIC miners, GPUs were initially very efficient at this.  
- ğŸ¤– **AI & Neural Networks:** Tensor cores accelerate matrix multiplications critical for training neural nets and powering generative AI.  
- ğŸ’¡ **Ray Tracing:** Specialized cores handle ray tracing calculations for realistic lighting and reflections in real-time graphics.
 
## **Micronâ€™s Role & Advancements**  
- ğŸ­ **Micron Memory Chips:** GDDR6X and future GDDR7 designed by Micron power high-speed data transfers on GPUs.  
- ğŸ”® **Innovations in Memory:** High Bandwidth Memory (HBM) for AI chips stacks DRAM vertically, creating high-capacity, high-throughput solutions at lower energy costs.
- ğŸ“š **Technological Marvel:** Modern graphics cards are a blend of advanced materials, clever architectures, and innovative manufacturing. They enable astonishing levels of visual realism, parallel computation, and AI capabilities.

[How do Graphics Cards Work? Exploring GPU Architecture](https://www.youtube.com/watch?v=h9Z4oGN89MU)