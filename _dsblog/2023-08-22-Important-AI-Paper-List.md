---
mathjax: true
id: 6090
title: Important AI Paper List
date: 2023-08-22
permalink: '/dsblog/aip'
tags: [LLM, Transformer, Encoder, Decoder, Encoder-Decoder, Machine Learning, NLP]
categories:


header:
    teaser: /assets/images/dspost/dsp6090-rps-Important-AI-Paper-List.jpg
excerpt_separator: "<!--more-->"  
excerpt:  
layout: single  
author_profile: true  
toc: True  
toc_sticky: true
---

![Important AI Paper List](/assets/images/dspost/dsp6090-rps-Important-AI-Paper-List.jpg)

# Introduciton

In almost all citations it becomes very difficult to read the title of research papers. Why? Because the contributors' information is first and most of the time, it is difficult to read the name other than native people. For example, if an Indian find a native name like "Vivek Ramaswami, Kartikeyan Karunanidhi" it is easy for them to read the name but the same name becomes difficult to read for non-Indian people, and vice-versa. Giving respect to the creator is very important but more than we need to know what they have done. I know from my experience, for almost every researcher, it becomes very difficult to track good AI research papers. For me, it is more difficult because I need to maintain this blog and I want to give references to the work across different webpages. Therefore I am creating a citation key, which includes the Last name of the first researcher + year of presenting that paper. Along with this, I am describing the title of the paper and where it was presented. If you find a particular title interesting for your work you can search that paper on "google scholar", Mendeley, sci-hub or other places with which you are familiar and comfortable. Post that you can download and read that paper at your leisure. Hope you find this list of some use for your work.


## Citations

###### [Bahdanau2015]
Neural machine translation by jointly learning to align and translate. In ICLR, 2015.
###### [Bao2020]
PLATO-2: towards building an open- domain chatbot via curriculum learning. arXiv preprint arXiv:2006.16779, 2020.
###### [Brown2020]
Language models are few-shot learners. In NeurIPS, 2020.
###### [Chen2020a]
Distilling knowledge learned in BERT for text generation. In ACL, 2020.
###### [Chen2020b]
Few-shot NLG with pre-trained language model. In ACL, 2020.
###### [Conneau2019]
Cross-lingual language model pretraining. In NeurIPS, 2019.
###### [Devlin2019]
BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.
###### [Dong2019]
Unified language model pretraining for natural language understanding and generation. In NeurIPS, 2019.
###### [Fan2019]
Unsupervised pre-training for sequence to sequence speech recognition. CoRR, arXiv preprint arXiv:1910.12418, 2019.
###### [Gehring2017]
Convolutional sequence to sequence learning. In ICML, 2017.
###### [Gong2020]
Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching. In COLING, 2020.
###### [Gu2020]
A tailored pre-training model for task-oriented dialog generation. arXiv preprint arXiv:2004.13835, 2020.
###### [Guan2020]
Survey on automatic text summarization and transformer models applicability. In CCRIS, 2020.
###### [Hendrycks2020]
Pretrained transformers improve out-of- distribution robustness. In ACL, 2020.
###### [Keskar2019]
CTRL: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.
###### [Kryscinski2018]
Improving abstraction in text summarization. In EMNLP, 2018.
###### [Lan2020]
ALBERT: A lite BERT for self-supervised learning of language representations. In ICLR, 2020.
###### [Lewis2020]
BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, 2020.
###### [Li2019]
Generating long and informative reviews with aspect-aware coarse-to-fine decoding. In ACL, pages 1969–1979, 2019.
###### [Li2020]
Knowledge-enhanced personalized review generation with capsule graph neural network. In CIKM, pages 735–744, 2020.
###### [Li2021a]
TextBox: A unified, modularized, and extensible framework for text generation. In ACL, 2021.
###### [Li2021b]
Few-shot knowledge graph-to-text generation with pretrained language models. In Findings of ACL, 2021.
###### [Li2021c]
Knowledge-based review generation by coherence enhanced text planning. In SIGIR, 2021.
###### [Lin2020]
Pretraining multilingual neural machine translation by leveraging alignment information. In EMNLP, 2020.
###### [Liu2019]
Text summarization with pretrained encoders. In EMNLP, 2019.
###### [Mager2020]
GPT-too: A language-model-first approach for AMR-to-text generation. In ACL, 2020.
###### [Peters2018]
Deep contextualized word representations. In NAACL-HLT, 2018.
###### [Qiu2020]
Pre-trained models for natural language processing: A survey. arXiv preprint arXiv:2003.08271, 2020.
###### [Radford2019]
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
###### [Raffel2020]
Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.
###### [Ribeiro2020]
Investigating pretrained language models for graph-to-text generation. arXiv preprint arXiv:2007.08426, 2020.
###### [Ross, 2012]
Guide for conducting risk assessments. In NIST Special Publication, 2012.
###### [Rothe2020]
Leveraging pre-trained checkpoints for sequence generation tasks. TACL, 2020.
###### [Sanh2019]
Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
###### [See2017]
Get to the point: Summarization with pointer-generator networks. In ACL, 2017.
###### [Song2019]
MASS: masked sequence to sequence pre-training for language generation. In ICML, 2019.
###### [Sun2019a]
Contrastive bidirectional transformer for temporal representation learning. arXiv preprint arXiv:1906.05743, 2019.
###### [Sun2019b]
Videobert: A joint model for video and language representation learning. In ICCV, 2019.
###### [Vaswani2017]
Attention is all you need. In NIPS, 2017.
###### [Wada2018]
Unsupervised cross-lingual word embedding by multilingual neural language models. arXiv preprint arXiv:1809.02306, 2018.
###### [Wolf2019]
Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv preprint arXiv:1901.08149, 2019.
###### [Xia2020]
XGPT: cross-modal generative pre-training for image captioning. arXiv preprint arXiv:2003.01473, 2020.
###### [Xu2020a]
Discourse-aware neural extractive text summarization. In ACL, 2020.
###### [Xu2020b]
Unsupervised extractive summarization by pre-training hierarchical transformers. In EMNLP, 2020.
###### [Yang2020a]
CSP: code-switching pre-training for neural machine translation. In EMNLP, 2020.
###### [Yang2020b]
TED: A pretrained unsupervised summarization model with theme modeling and denoising. In EMNLP (Findings), 2020.
###### [Zaib2020]
A short survey of pre-trained language models for conversational AI-A new age in NLP. In ACSW, 2020.
###### [Zeng2020]
Generalized conditioned dialogue generation based on pre-trained language model. arXiv preprint arXiv:2010.11140, 2020.
###### [Zhang2019a]
Pretraining-based natural language generation for text summarization. In CoNLL, 2019.
###### [Zhang2019b]
HIBERT: document level pre-training of hierarchical bidirectional transformers for document summarization. In ACL, 2019.
###### [Zhang2019c]
ERNIE: enhanced language representation with informative entities. In ACL, 2019.
###### [Zhang2020]
DIALOGPT : Largescale generative pre-training for conversational response generation. In ACL, 2020.
###### [Zhao2020]
Knowledge-grounded dialogue generation with pretrained language models. In EMNLP, 2020.
###### [Zheng2019]
Sentence centrality revisited for unsupervised summarization. In ACL, 2019.
###### [Zhou2020]
Unified vision-language pre-training for image captioning and VQA. In AAAI, 2020